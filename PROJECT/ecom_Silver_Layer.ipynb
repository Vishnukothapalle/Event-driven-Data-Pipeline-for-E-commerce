{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery client initialized successfully!\n",
      "Dataset 'ecommerce_Silver_Layer' already exists.\n",
      "\n",
      "======================================================================\n",
      "PROCESSING: DIM_CUSTOMER\n",
      "======================================================================\n",
      "   Extracted 99,441 rows from Bronze\n",
      "   Transforming dim_customer... (initial rows: 99,441)\n",
      "   â†’ Removed 0 duplicate rows (now 99,441 rows)\n",
      "   â†’ Filled 278 NULLs in 'Latitude' with median: -22.93\n",
      "   â†’ Filled 278 NULLs in 'Longitude' with median: -46.63\n",
      "   â†’ Total numeric NULLs filled (median): 556\n",
      "   â†’ Added 'load_timestamp' to all 99,441 rows\n",
      "   ðŸ“Š Summary for dim_customer: 556 total changes applied\n",
      "   â†’ Final rows: 99,441 (from 99,441)\n",
      "   Successfully loaded 99,441 rows into Silver `dim_customer`\n",
      "\n",
      "======================================================================\n",
      "PROCESSING: DIM_ORDER\n",
      "======================================================================\n",
      "   Extracted 99,441 rows from Bronze\n",
      "   Transforming dim_order... (initial rows: 99,441)\n",
      "   â†’ Removed 0 duplicate rows (now 99,441 rows)\n",
      "   â†’ Filled 160 NULLs in 'order_approved_at' with 'Unknown'\n",
      "   â†’ Filled 1,783 NULLs in 'order_delivered_carrier_date' with 'Unknown'\n",
      "   â†’ Filled 2,965 NULLs in 'order_delivered_customer_date' with 'Unknown'\n",
      "   â†’ Total string NULLs filled: 4,908\n",
      "   â†’ Added 'load_timestamp' to all 99,441 rows\n",
      "   ðŸ“Š Summary for dim_order: 4,908 total changes applied\n",
      "   â†’ Final rows: 99,441 (from 99,441)\n",
      "   Successfully loaded 99,441 rows into Silver `dim_order`\n",
      "\n",
      "======================================================================\n",
      "PROCESSING: DIM_PAYMENTS\n",
      "======================================================================\n",
      "   Extracted 103,886 rows from Bronze\n",
      "   Transforming dim_payments... (initial rows: 103,886)\n",
      "   â†’ Removed 2,050 duplicate rows (now 101,836 rows)\n",
      "   â†’ Added 'load_timestamp' to all 101,836 rows\n",
      "   ðŸ“Š Summary for dim_payments: 2,050 total changes applied\n",
      "   â†’ Final rows: 101,836 (from 103,886)\n",
      "   Successfully loaded 101,836 rows into Silver `dim_payments`\n",
      "\n",
      "======================================================================\n",
      "PROCESSING: DIM_SELLERS\n",
      "======================================================================\n",
      "   Extracted 3,095 rows from Bronze\n",
      "   Transforming dim_sellers... (initial rows: 3,095)\n",
      "   â†’ Removed 0 duplicate rows (now 3,095 rows)\n",
      "   â†’ Filled 7 NULLs in 'Latitude' with median: -23.49\n",
      "   â†’ Filled 7 NULLs in 'Longitude' with median: -46.80\n",
      "   â†’ Total numeric NULLs filled (median): 14\n",
      "   â†’ Added 'load_timestamp' to all 3,095 rows\n",
      "   ðŸ“Š Summary for dim_sellers: 14 total changes applied\n",
      "   â†’ Final rows: 3,095 (from 3,095)\n",
      "   Successfully loaded 3,095 rows into Silver `dim_sellers`\n",
      "\n",
      "======================================================================\n",
      "PROCESSING: DIM_PRODUCTS\n",
      "======================================================================\n",
      "   Extracted 32,951 rows from Bronze\n",
      "   Transforming dim_products... (initial rows: 32,951)\n",
      "   â†’ Removed 0 duplicate rows (now 32,951 rows)\n",
      "   â†’ Filled 610 NULLs in 'product_name_lenght' with median: 51.00\n",
      "   â†’ Filled 610 NULLs in 'product_description_lenght' with median: 595.00\n",
      "   â†’ Filled 610 NULLs in 'product_photos_qty' with median: 1.00\n",
      "   â†’ Filled 2 NULLs in 'product_weight_g' with median: 700.00\n",
      "   â†’ Filled 2 NULLs in 'product_length_cm' with median: 25.00\n",
      "   â†’ Filled 2 NULLs in 'product_height_cm' with median: 13.00\n",
      "   â†’ Filled 2 NULLs in 'product_width_cm' with median: 20.00\n",
      "   â†’ Total numeric NULLs filled (median): 1,838\n",
      "   â†’ Filled 610 NULLs in 'product_category_name' with 'Unknown'\n",
      "   â†’ Total string NULLs filled: 610\n",
      "   â†’ Added 'load_timestamp' to all 32,951 rows\n",
      "   ðŸ“Š Summary for dim_products: 2,448 total changes applied\n",
      "   â†’ Final rows: 32,951 (from 32,951)\n",
      "   Successfully loaded 32,951 rows into Silver `dim_products`\n",
      "\n",
      "======================================================================\n",
      "PROCESSING: SYNTHETIC_ORDER_LIFECYCLE\n",
      "======================================================================\n",
      "   Extracted 5,052 rows from Bronze\n",
      "   Transforming synthetic_order_lifecycle... (initial rows: 5,052)\n",
      "   â†’ Removed 0 duplicate rows (now 5,052 rows)\n",
      "   â†’ [SYNTHETIC] Filled 3,789 NULLs in 'payment_value' with 0\n",
      "   â†’ Total numeric NULLs filled (0 (synthetic)): 3,789\n",
      "   â†’ Added 'load_timestamp' to all 5,052 rows\n",
      "   â†’ Converting 'event_timestamp' to UTC\n",
      "   â†’ Added 'days_since_event' to 5,052 valid rows\n",
      "   ðŸ“Š Summary for synthetic_order_lifecycle: 3,789 total changes applied\n",
      "   â†’ Final rows: 5,052 (from 5,052)\n",
      "   Silver table `synthetic_order_lifecycle` created.\n",
      "   Successfully loaded 5,052 rows into Silver `synthetic_order_lifecycle`\n",
      "\n",
      "ALL DONE! Bronze to Silver ETL completed.\n",
      "Note: 'synthetic_order_lifecycle' numeric NULLs filled with 0, others with median.\n"
     ]
    }
   ],
   "source": [
    "# Bronze to Silver Layer (Pandas + Timezone-Safe + Change Counts + Custom Fill for synthetic_order_lifecycle)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.api_core.exceptions import Conflict, NotFound\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Setup & Authentication\n",
    "# ----------------------------\n",
    "KEY_PATH = r\"C:\\Users\\Vishnu Vardhan\\OneDrive\\Desktop\\Bigquery_Ecommerce\\even-blueprint-441418-p2-043f8a9d855b.json\"\n",
    "\n",
    "PROJECT_ID = \"even-blueprint-441418-p2\"\n",
    "BRONZE_DATASET_ID = \"ecommerce_Bronze_Layer\" \n",
    "SILVER_DATASET_ID = \"ecommerce_Silver_Layer\" \n",
    "\n",
    "# Authenticate\n",
    "credentials = service_account.Credentials.from_service_account_file(KEY_PATH)\n",
    "client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
    "\n",
    "print(\"BigQuery client initialized successfully!\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Create Silver Dataset\n",
    "# ----------------------------\n",
    "dataset_ref = bigquery.Dataset(f\"{PROJECT_ID}.{SILVER_DATASET_ID}\")\n",
    "dataset_ref.location = \"US\"\n",
    "\n",
    "try:\n",
    "    client.create_dataset(dataset_ref)\n",
    "    print(f\"Dataset '{SILVER_DATASET_ID}' created.\")\n",
    "except Conflict:\n",
    "    print(f\"Dataset '{SILVER_DATASET_ID}' already exists.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Define Tables & Duration Logic\n",
    "# ----------------------------\n",
    "TABLES = [\n",
    "    \"dim_customer\",\n",
    "    \"dim_order\",\n",
    "    \"dim_payments\",\n",
    "    \"dim_sellers\",\n",
    "    \"dim_products\",\n",
    "    \"synthetic_order_lifecycle\"\n",
    "]\n",
    "\n",
    "DATE_DURATION_CONFIG = {\n",
    "    \"dim_orders\": (\"order_purchase_timestamp\", \"order_delivered_customer_date\"),\n",
    "    \"synthetic_order_lifecycle\": (\"event_timestamp\", None)\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Safe Datetime Conversion\n",
    "# ----------------------------\n",
    "def to_utc_datetime(series: pd.Series, col_name: str) -> pd.Series:\n",
    "    if series.dtype == 'object':\n",
    "        series = pd.to_datetime(series, errors='coerce', utc=True)\n",
    "    elif pd.api.types.is_datetime64_any_dtype(series):\n",
    "        if series.dt.tz is None:\n",
    "            series = series.dt.tz_localize('UTC')\n",
    "        else:\n",
    "            series = series.dt.tz_convert('UTC')\n",
    "    else:\n",
    "        series = pd.to_datetime(series, errors='coerce', utc=True)\n",
    "    \n",
    "    if series.isnull().all():\n",
    "        print(f\"   Warning: '{col_name}' has all NULL/invalid dates\")\n",
    "    return series\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Pandas Transformation Function (Custom Fill Logic)\n",
    "# ----------------------------\n",
    "def transform_with_pandas(df: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "    print(f\"   Transforming {table_name}... (initial rows: {len(df):,})\")\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    total_changes = 0\n",
    "\n",
    "    # 1. Remove duplicates\n",
    "    dup_count = len(df) - len(df.drop_duplicates())\n",
    "    df = df.drop_duplicates()\n",
    "    total_changes += dup_count\n",
    "    print(f\"   â†’ Removed {dup_count:,} duplicate rows (now {len(df):,} rows)\")\n",
    "\n",
    "    # 2. Fill numeric NULLs â†’ **0 for synthetic_order_lifecycle, else median**\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    num_fills_total = 0\n",
    "    for col in numeric_cols:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        if null_count > 0:\n",
    "            if table_name == \"synthetic_order_lifecycle\":\n",
    "                fill_value = 0\n",
    "                df[col].fillna(0, inplace=True)\n",
    "                print(f\"   â†’ [SYNTHETIC] Filled {null_count:,} NULLs in '{col}' with 0\")\n",
    "            else:\n",
    "                median_val = df[col].median()\n",
    "                df[col].fillna(median_val, inplace=True)\n",
    "                print(f\"   â†’ Filled {null_count:,} NULLs in '{col}' with median: {median_val:.2f}\")\n",
    "                fill_value = median_val\n",
    "            num_fills_total += null_count\n",
    "\n",
    "    if num_fills_total > 0:\n",
    "        fill_type = \"0 (synthetic)\" if table_name == \"synthetic_order_lifecycle\" else \"median\"\n",
    "        print(f\"   â†’ Total numeric NULLs filled ({fill_type}): {num_fills_total:,}\")\n",
    "        total_changes += num_fills_total\n",
    "\n",
    "    # 3. Fill string NULLs\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    str_fills_total = 0\n",
    "    zip_fills = 0\n",
    "    for col in string_cols:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        if null_count > 0:\n",
    "            if 'postal' in col.lower() or 'zip' in col.lower():\n",
    "                df[col].fillna(0, inplace=True)\n",
    "                zip_fills += null_count\n",
    "                print(f\"   â†’ Filled {null_count:,} NULLs in '{col}' with 0\")\n",
    "            else:\n",
    "                df[col].fillna('Unknown', inplace=True)\n",
    "                str_fills_total += null_count\n",
    "                print(f\"   â†’ Filled {null_count:,} NULLs in '{col}' with 'Unknown'\")\n",
    "\n",
    "    total_changes += str_fills_total + zip_fills\n",
    "    if str_fills_total > 0:\n",
    "        print(f\"   â†’ Total string NULLs filled: {str_fills_total:,}\")\n",
    "    if zip_fills > 0:\n",
    "        print(f\"   â†’ Total zip/postal NULLs filled: {zip_fills:,}\")\n",
    "\n",
    "    # 4. Add load timestamp\n",
    "    df['load_timestamp'] = pd.Timestamp.now(tz='UTC')\n",
    "    print(f\"   â†’ Added 'load_timestamp' to all {len(df):,} rows\")\n",
    "\n",
    "    # 5. Duration calculation\n",
    "    date_drops = 0\n",
    "    if table_name in DATE_DURATION_CONFIG:\n",
    "        start_col, end_col = DATE_DURATION_CONFIG[table_name]\n",
    "        \n",
    "        if start_col in df.columns:\n",
    "            print(f\"   â†’ Converting '{start_col}' to UTC\")\n",
    "            df[start_col] = to_utc_datetime(df[start_col], start_col)\n",
    "            \n",
    "            if end_col and end_col in df.columns:\n",
    "                print(f\"   â†’ Converting '{end_col}' to UTC\")\n",
    "                df[end_col] = to_utc_datetime(df[end_col], end_col)\n",
    "                \n",
    "                mask = df[start_col].notna() & df[end_col].notna()\n",
    "                date_drops = (~mask).sum()\n",
    "                df = df[mask].copy()\n",
    "                total_changes += date_drops\n",
    "                if date_drops > 0:\n",
    "                    print(f\"   â†’ Dropped {date_drops:,} rows with invalid dates\")\n",
    "                \n",
    "                df['duration_days'] = (df[end_col] - df[start_col]).dt.days\n",
    "                print(f\"   â†’ Added 'duration_days' to {len(df):,} valid rows\")\n",
    "            else:\n",
    "                mask = df[start_col].notna()\n",
    "                date_drops = (~mask).sum()\n",
    "                df = df[mask].copy()\n",
    "                total_changes += date_drops\n",
    "                if date_drops > 0:\n",
    "                    print(f\"   â†’ Dropped {date_drops:,} rows with invalid '{start_col}'\")\n",
    "                now_utc = pd.Timestamp.now(tz='UTC')\n",
    "                df['days_since_event'] = (now_utc - df[start_col]).dt.days\n",
    "                print(f\"   â†’ Added 'days_since_event' to {len(df):,} valid rows\")\n",
    "\n",
    "    # Final Summary\n",
    "    print(f\"   ðŸ“Š Summary for {table_name}: {total_changes:,} total changes applied\")\n",
    "    print(f\"   â†’ Final rows: {len(df):,} (from {initial_rows:,})\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Main ETL Loop\n",
    "# ----------------------------\n",
    "for table_name in TABLES:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROCESSING: {table_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # --- Extract ---\n",
    "    bronze_table = f\"{PROJECT_ID}.{BRONZE_DATASET_ID}.{table_name}\"\n",
    "    try:\n",
    "        query = f\"SELECT * FROM `{bronze_table}`\"\n",
    "        df = client.query(query).to_dataframe()\n",
    "        print(f\"   Extracted {len(df):,} rows from Bronze\")\n",
    "    except NotFound:\n",
    "        print(f\"   Table `{table_name}` not found in Bronze. Skipping.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"   Error querying {table_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"   DataFrame is empty. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # --- Transform ---\n",
    "    try:\n",
    "        df_clean = transform_with_pandas(df.copy(), table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"   Transformation failed for {table_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # --- Load ---\n",
    "    silver_table = f\"{PROJECT_ID}.{SILVER_DATASET_ID}.{table_name}\"\n",
    "\n",
    "    try:\n",
    "        table = bigquery.Table(silver_table)\n",
    "        client.create_table(table)\n",
    "        print(f\"   Silver table `{table_name}` created.\")\n",
    "    except Conflict:\n",
    "        pass\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        write_disposition=\"WRITE_TRUNCATE\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(df_clean, silver_table, job_config=job_config)\n",
    "        job.result()\n",
    "        final_rows = client.get_table(silver_table).num_rows\n",
    "        print(f\"   Successfully loaded {final_rows:,} rows into Silver `{table_name}`\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Failed to load {table_name}: {e}\")\n",
    "\n",
    "print(\"\\nALL DONE! Bronze to Silver ETL completed.\")\n",
    "print(\"Note: 'synthetic_order_lifecycle' numeric NULLs filled with 0, others with median.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b212f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
